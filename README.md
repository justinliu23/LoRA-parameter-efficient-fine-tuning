# LoRA-parameter-efficient-fine-tuning

This repository provides an implementation for fine-tuning language models using Low-Rank Adaptation (LoRA) techniques. LoRA is an efficient method for adapting pre-trained models with a reduced number of trainable parameters, which can significantly cut down computational costs and required training time.

### Prerequisites

To run the notebook, you need the following installed:

- Python 3.7+
- PyTorch

## References
- **LoRA Paper**: Hu, E., et al., "LoRA: Low-Rank Adaptation of Large Language Models." [Link to paper](https://arxiv.org/abs/2106.09685).
